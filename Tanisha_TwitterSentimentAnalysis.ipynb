{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given Twitter US Airline Sentiment Dataset, which contains data for over 14000 tweets, your task is to predict the sentiment of the tweet i.e. positive, negative or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk import word_tokenize \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Anisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@SouthwestAir I am scheduled for the morning, 2 days after the fact, yes..not sure why my evening flight was the only one Cancelled Flightled'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data=pd.read_csv('training_twitter_x_y_train.csv')\n",
    "training_data[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@AmericanAir In car gng to DFW. Pulled over 1hr ago - very icy roads. On-hold with AA since 1hr. Can't reach arpt for AA2450. Wat 2 do?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data=pd.read_csv(\"test_twitter_x_test.csv\")\n",
    "testing_data[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=training_data[\"text\"]\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3660, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest=testing_data[\"text\"]\n",
    "xtest=np.array(xtest)\n",
    "xtest=xtest.reshape(len(xtest),1)\n",
    "xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10980, 1), (10980, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=training_data['airline_sentiment']\n",
    "x=np.array(x)\n",
    "y=np.array(y)\n",
    "x=x.reshape(len(x),1)\n",
    "y=y.reshape(len(y),1)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10980, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=np.append(x,y,axis=1)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3660, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=xtest\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['@',\n",
       "   'SouthwestAir',\n",
       "   'I',\n",
       "   'am',\n",
       "   'scheduled',\n",
       "   'for',\n",
       "   'the',\n",
       "   'morning',\n",
       "   ',',\n",
       "   '2',\n",
       "   'days',\n",
       "   'after',\n",
       "   'the',\n",
       "   'fact',\n",
       "   ',',\n",
       "   'yes..not',\n",
       "   'sure',\n",
       "   'why',\n",
       "   'my',\n",
       "   'evening',\n",
       "   'flight',\n",
       "   'was',\n",
       "   'the',\n",
       "   'only',\n",
       "   'one',\n",
       "   'Cancelled',\n",
       "   'Flightled'],\n",
       "  'negative'),\n",
       " (['@',\n",
       "   'SouthwestAir',\n",
       "   'seeing',\n",
       "   'your',\n",
       "   'workers',\n",
       "   'time',\n",
       "   'in',\n",
       "   'and',\n",
       "   'time',\n",
       "   'out',\n",
       "   'going',\n",
       "   'above',\n",
       "   'and',\n",
       "   'beyond',\n",
       "   'is',\n",
       "   'why',\n",
       "   'I',\n",
       "   'love',\n",
       "   'flying',\n",
       "   'with',\n",
       "   'you',\n",
       "   'guys',\n",
       "   '.',\n",
       "   'Thank',\n",
       "   'you',\n",
       "   '!'],\n",
       "  'positive')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "c=0\n",
    "for text,category in train:\n",
    "    c+=1\n",
    "    documents.append((word_tokenize(text), category))\n",
    "documents[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[',\n",
       "  '``',\n",
       "  '@',\n",
       "  'AmericanAir',\n",
       "  'In',\n",
       "  'car',\n",
       "  'gng',\n",
       "  'to',\n",
       "  'DFW',\n",
       "  '.',\n",
       "  'Pulled',\n",
       "  'over',\n",
       "  '1hr',\n",
       "  'ago',\n",
       "  '-',\n",
       "  'very',\n",
       "  'icy',\n",
       "  'roads',\n",
       "  '.',\n",
       "  'On-hold',\n",
       "  'with',\n",
       "  'AA',\n",
       "  'since',\n",
       "  '1hr',\n",
       "  '.',\n",
       "  'Ca',\n",
       "  \"n't\",\n",
       "  'reach',\n",
       "  'arpt',\n",
       "  'for',\n",
       "  'AA2450',\n",
       "  '.',\n",
       "  'Wat',\n",
       "  '2',\n",
       "  'do',\n",
       "  '?',\n",
       "  \"''\",\n",
       "  ']'],\n",
       " ['[',\n",
       "  \"'\",\n",
       "  '@',\n",
       "  'AmericanAir',\n",
       "  'after',\n",
       "  'all',\n",
       "  ',',\n",
       "  'the',\n",
       "  'plane',\n",
       "  'didn',\n",
       "  'â€™',\n",
       "  't',\n",
       "  'land',\n",
       "  'in',\n",
       "  'identical',\n",
       "  'or',\n",
       "  'worse',\n",
       "  ')',\n",
       "  'conditions',\n",
       "  'at',\n",
       "  'GRK',\n",
       "  'according',\n",
       "  'to',\n",
       "  'METARs',\n",
       "  '.',\n",
       "  \"'\",\n",
       "  ']']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_documents = []\n",
    "c=0\n",
    "for text in test:\n",
    "    c+=1\n",
    "    test_documents.append(word_tokenize(str(text)))\n",
    "test_documents[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['@',\n",
       "   'USAirways',\n",
       "   'husband',\n",
       "   'tried',\n",
       "   'to',\n",
       "   'use',\n",
       "   'dividends',\n",
       "   '&',\n",
       "   'amp',\n",
       "   ';',\n",
       "   'companion',\n",
       "   'fair',\n",
       "   'for',\n",
       "   'emergency',\n",
       "   ',',\n",
       "   'transferred',\n",
       "   '&',\n",
       "   'amp',\n",
       "   ';',\n",
       "   'put',\n",
       "   'on',\n",
       "   'hold',\n",
       "   'for',\n",
       "   'over',\n",
       "   'two',\n",
       "   'hours',\n",
       "   'gave',\n",
       "   'up',\n",
       "   '#',\n",
       "   'disappointed'],\n",
       "  'negative'),\n",
       " (['@',\n",
       "   'united-rebooked',\n",
       "   'to',\n",
       "   'OMA',\n",
       "   '-',\n",
       "   '180',\n",
       "   'miles',\n",
       "   'from',\n",
       "   'my',\n",
       "   'destination',\n",
       "   '.',\n",
       "   'Spotty',\n",
       "   'customer',\n",
       "   'service',\n",
       "   '.',\n",
       "   'I',\n",
       "   'get',\n",
       "   'staff',\n",
       "   'stressors',\n",
       "   'but',\n",
       "   'come',\n",
       "   'on',\n",
       "   ',',\n",
       "   'this',\n",
       "   'is',\n",
       "   'your',\n",
       "   'business',\n",
       "   '.'],\n",
       "  'negative')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(documents)\n",
    "documents[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_simple_pos(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('better', 'RBR')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "w = \"better\"\n",
    "pos_tag([w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stops = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "stops.update(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(words):\n",
    "    output_words = []\n",
    "    #regex = re.compile('[a-zA-Z0-9_-]+$')\n",
    "    regex = re.compile('[a-zA-Z_-]+$')\n",
    "    for w in words:\n",
    "        if w.lower() not in stops and len(w)>2 and not w.isnumeric() and re.match(regex,w):\n",
    "            pos = pos_tag([w])\n",
    "            clean_word = lemmatizer.lemmatize(w, pos = get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(clean_review(document), category) for document, category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents = [clean_review(document) for document in test_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_documents = documents\n",
    "testing_documents = test_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [category for document, category in training_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining All features in each tuple for Train Data\n",
    "text_documents = [\" \".join(document) for document, category in training_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining All features in each tuple for Test Data\n",
    "test_text_documents = [\" \".join(document) for document in testing_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#x_train, x_test, y_train, y_test = train_test_split(text_documents, categories)\n",
    "#count_vec = CountVectorizer(max_features = 2000, ngram_range=(1,2))\n",
    "count_vec = CountVectorizer(max_features = 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_features = count_vec.fit_transform(text_documents)\n",
    "x_train_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10980, 7000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='Coding'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile('[a-zA-Z_-]+$')\n",
    "re.match(regex,'Coding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__rwg__',\n",
       " '_austrian',\n",
       " '_defcon_',\n",
       " '_emmaclifford',\n",
       " '_exact_',\n",
       " '_justdippin_',\n",
       " 'a_life_story_',\n",
       " 'aa',\n",
       " 'aaaand',\n",
       " 'aadvantage',\n",
       " 'aafail',\n",
       " 'aal',\n",
       " 'aaron',\n",
       " 'aarp',\n",
       " 'abandon',\n",
       " 'abandonment',\n",
       " 'abassinet',\n",
       " 'abbreve',\n",
       " 'abc',\n",
       " 'abcnetwork',\n",
       " 'abcnews',\n",
       " 'abduct',\n",
       " 'abi',\n",
       " 'abigailedge',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'aboard',\n",
       " 'aboout',\n",
       " 'abounds',\n",
       " 'abq',\n",
       " 'abroad',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absorber',\n",
       " 'absoulutely',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'abt',\n",
       " 'abundance',\n",
       " 'abuse',\n",
       " 'abysmal',\n",
       " 'acc',\n",
       " 'accelerate',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accepted',\n",
       " 'acces',\n",
       " 'access',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'accidents',\n",
       " 'accomidating',\n",
       " 'accommodate',\n",
       " 'accommodation',\n",
       " 'accompaniment',\n",
       " 'accompany',\n",
       " 'accomplish',\n",
       " 'accord',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accrue',\n",
       " 'acct',\n",
       " 'accts',\n",
       " 'accumulation',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accuratetraveltimes',\n",
       " 'accuse',\n",
       " 'achieve',\n",
       " 'ack',\n",
       " 'acknowledge',\n",
       " 'acknowledgement',\n",
       " 'acknowledgment',\n",
       " 'acnewsguy',\n",
       " 'acosta',\n",
       " 'acoustic',\n",
       " 'acpt',\n",
       " 'acquire',\n",
       " 'across',\n",
       " 'act',\n",
       " 'actingoutmgmnt',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'active',\n",
       " 'active_aly',\n",
       " 'actively',\n",
       " 'activity',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'actualy',\n",
       " 'acu',\n",
       " 'acy',\n",
       " 'ad',\n",
       " 'ada',\n",
       " 'adam',\n",
       " 'adam_karren',\n",
       " 'adapt',\n",
       " 'add',\n",
       " 'addair',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additonal',\n",
       " 'addr',\n",
       " 'address',\n",
       " 'adds',\n",
       " 'addtl',\n",
       " 'adjacent',\n",
       " 'adjustment',\n",
       " 'admin',\n",
       " 'admiral',\n",
       " 'admirals',\n",
       " 'admiralsclub',\n",
       " 'admit',\n",
       " 'adolfo',\n",
       " 'adopt',\n",
       " 'adopting',\n",
       " 'adore',\n",
       " 'adult',\n",
       " 'adv',\n",
       " 'advan',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'adventure',\n",
       " 'advertise',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advis',\n",
       " 'advise',\n",
       " 'advisory',\n",
       " 'advsry',\n",
       " 'aerocivilcol',\n",
       " 'aerojobmarket',\n",
       " 'aeroport',\n",
       " 'aex',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affiliate',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'afiliates',\n",
       " 'aflame',\n",
       " 'afraid',\n",
       " 'african',\n",
       " 'aft',\n",
       " 'afterall',\n",
       " 'afternoon',\n",
       " 'aftr',\n",
       " 'again',\n",
       " 'age',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'aggiemensgolf',\n",
       " 'aggravate',\n",
       " 'aggravation',\n",
       " 'aggressive',\n",
       " 'agnt',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreement',\n",
       " 'agt',\n",
       " 'agts',\n",
       " 'aha',\n",
       " 'ahead',\n",
       " 'ahhhh',\n",
       " 'ahold',\n",
       " 'ahoy',\n",
       " 'ail',\n",
       " 'aim',\n",
       " 'air',\n",
       " 'airborne',\n",
       " 'airbus',\n",
       " 'airbusintheus',\n",
       " 'aircanada',\n",
       " 'aircargo',\n",
       " 'aircraft',\n",
       " 'aires',\n",
       " 'airfare',\n",
       " 'airfarewatchdog',\n",
       " 'airline',\n",
       " 'airlinegeeks',\n",
       " 'airlineguys',\n",
       " 'airlinequality',\n",
       " 'airliner',\n",
       " 'airlines',\n",
       " 'airlinesecurity',\n",
       " 'airnzusa',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airportcardio',\n",
       " 'airpt',\n",
       " 'airside',\n",
       " 'airstairs',\n",
       " 'airway',\n",
       " 'airways',\n",
       " 'ais',\n",
       " 'aisle',\n",
       " 'aka',\n",
       " 'ala',\n",
       " 'alabama',\n",
       " 'alamo',\n",
       " 'alan_bledsoe',\n",
       " 'alarm',\n",
       " 'alaska',\n",
       " 'alaskaair',\n",
       " 'alavera',\n",
       " 'alb',\n",
       " 'albany',\n",
       " 'albanyairport',\n",
       " 'albeit',\n",
       " 'albertbreer',\n",
       " 'album',\n",
       " 'albuquer',\n",
       " 'albuquerque',\n",
       " 'alcohol',\n",
       " 'alert',\n",
       " 'alex',\n",
       " 'alfamilyoffour',\n",
       " 'ali',\n",
       " 'alicia',\n",
       " 'align',\n",
       " 'alison',\n",
       " 'alist',\n",
       " 'alittlebetter',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allan',\n",
       " 'allegiantair',\n",
       " 'allegianttravel',\n",
       " 'allende',\n",
       " 'allergic',\n",
       " 'allergy',\n",
       " 'alleviate',\n",
       " 'allgood',\n",
       " 'alliance',\n",
       " 'allow',\n",
       " 'allowabl',\n",
       " 'allowable',\n",
       " 'allowance',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allready',\n",
       " 'allyoucanjetpass',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alreadyrebookedonce',\n",
       " 'alright',\n",
       " 'als',\n",
       " 'also',\n",
       " 'alsonodrinkcartcomingaround',\n",
       " 'alstdi',\n",
       " 'alt',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'alternatively',\n",
       " 'although',\n",
       " 'altitude',\n",
       " 'altonbrownlive',\n",
       " 'always',\n",
       " 'alwaysdelayed',\n",
       " 'alwayshappensthere',\n",
       " 'alwayslate',\n",
       " 'alynewton',\n",
       " 'am',\n",
       " 'amagrino',\n",
       " 'amarillo',\n",
       " 'amateur',\n",
       " 'amateurish',\n",
       " 'amaze',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazings',\n",
       " 'amazon',\n",
       " 'ambivalence',\n",
       " 'amen',\n",
       " 'amenity',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americanair',\n",
       " 'americanairbr',\n",
       " 'americanairlines',\n",
       " 'americanairlnes',\n",
       " 'americanairsucks',\n",
       " 'americanforlife',\n",
       " 'americanone',\n",
       " 'americant',\n",
       " 'americanview',\n",
       " 'amex',\n",
       " 'amin_aur',\n",
       " 'amirite',\n",
       " 'amm',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'ams',\n",
       " 'amsterdam',\n",
       " 'amt',\n",
       " 'amy',\n",
       " 'amybruni',\n",
       " 'amypoehler',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'analystdoc',\n",
       " 'analysts',\n",
       " 'analytics',\n",
       " 'anamarketers',\n",
       " 'anaphylaxis',\n",
       " 'anarchy',\n",
       " 'anchorage',\n",
       " 'andchexmix',\n",
       " 'anderson',\n",
       " 'andrew',\n",
       " 'andrew_wasila',\n",
       " 'andrewbiga',\n",
       " 'andrewfallis',\n",
       " 'andrews',\n",
       " 'android',\n",
       " 'andthewinneris',\n",
       " 'andyellwood',\n",
       " 'angel',\n",
       " 'angeles',\n",
       " 'angle',\n",
       " 'angriest',\n",
       " 'angry',\n",
       " 'angryandsober',\n",
       " 'angrybird',\n",
       " 'angrycustomer',\n",
       " 'angrytraveler',\n",
       " 'angst',\n",
       " 'anhour',\n",
       " 'animal',\n",
       " 'anku',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'annebevi',\n",
       " 'annettenaif',\n",
       " 'anni',\n",
       " 'anniversary',\n",
       " 'annnndddd',\n",
       " 'annnnddddd',\n",
       " 'annnnnd',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announces',\n",
       " 'annoy',\n",
       " 'annoyed',\n",
       " 'annricord',\n",
       " 'annual',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answering',\n",
       " 'answerphone',\n",
       " 'answerthephone',\n",
       " 'answerthis',\n",
       " 'ant_kneee',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'anticipate',\n",
       " 'anticipation',\n",
       " 'antigua',\n",
       " 'antitrust',\n",
       " 'antonio',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhelp',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyonethere',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'apathetic',\n",
       " 'apathy',\n",
       " 'apeared',\n",
       " 'apnea',\n",
       " 'apollochplayers',\n",
       " 'apologise',\n",
       " 'apologize',\n",
       " 'apologizes',\n",
       " 'apology',\n",
       " 'apostrophe',\n",
       " 'apostrophefail',\n",
       " 'app',\n",
       " 'appal',\n",
       " 'appalled',\n",
       " 'appalling',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appears',\n",
       " 'appease',\n",
       " 'applaud',\n",
       " 'applause',\n",
       " 'apple',\n",
       " 'applepay',\n",
       " 'appleton',\n",
       " 'applicable',\n",
       " 'application',\n",
       " 'applied',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'appoint',\n",
       " 'appointment',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciates',\n",
       " 'appreciation',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'appropriately',\n",
       " 'appropriation',\n",
       " 'approval',\n",
       " 'approve',\n",
       " 'approx',\n",
       " 'approximate',\n",
       " 'approximately',\n",
       " 'apps',\n",
       " 'appt',\n",
       " 'april',\n",
       " 'apron',\n",
       " 'apt',\n",
       " 'apx',\n",
       " 'aquadilla',\n",
       " 'arab',\n",
       " 'arbitrarily',\n",
       " 'arbitrary',\n",
       " 'arc',\n",
       " 'ardent',\n",
       " 'are',\n",
       " 'area',\n",
       " 'arent',\n",
       " 'areyounew',\n",
       " 'argentina',\n",
       " 'argg',\n",
       " 'argh',\n",
       " 'argue',\n",
       " 'argument',\n",
       " 'arizona',\n",
       " 'arkansas',\n",
       " 'arm',\n",
       " 'arminrosen',\n",
       " 'armrest',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'arose',\n",
       " 'around',\n",
       " 'arrange',\n",
       " 'arrangement',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'arrogant',\n",
       " 'art',\n",
       " 'article',\n",
       " 'artisanal',\n",
       " 'aruba',\n",
       " 'aruba_airport',\n",
       " 'aruna',\n",
       " 'as',\n",
       " 'asap',\n",
       " 'ase',\n",
       " 'ash',\n",
       " 'asha',\n",
       " 'ashamed',\n",
       " 'ashley',\n",
       " 'ashleykatherton',\n",
       " 'asia',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'askpaypal',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspen',\n",
       " 'ass',\n",
       " 'assault',\n",
       " 'asset',\n",
       " 'asshole',\n",
       " 'assign',\n",
       " 'assigned',\n",
       " 'assignment',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assistant',\n",
       " 'associate',\n",
       " 'assult',\n",
       " 'assume',\n",
       " 'assumed',\n",
       " 'assurance',\n",
       " 'assure',\n",
       " 'assured',\n",
       " 'astound',\n",
       " 'astounds',\n",
       " 'at',\n",
       " 'atc',\n",
       " 'atct',\n",
       " 'ath',\n",
       " 'athlete',\n",
       " 'atl',\n",
       " 'atlanta',\n",
       " 'atlantic',\n",
       " 'atleast',\n",
       " 'atrocious',\n",
       " 'att',\n",
       " 'attach',\n",
       " 'attain',\n",
       " 'attdt',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attend',\n",
       " 'attendant',\n",
       " 'attendants',\n",
       " 'attendee',\n",
       " 'attendents',\n",
       " 'attention',\n",
       " 'attentiveness',\n",
       " 'attitude',\n",
       " 'attitudeissues',\n",
       " 'attndt',\n",
       " 'atwonline',\n",
       " 'auciello',\n",
       " 'auckland',\n",
       " 'auction',\n",
       " 'audience',\n",
       " 'audition',\n",
       " 'auditorium',\n",
       " 'auf',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'auh',\n",
       " 'aunt',\n",
       " 'aunty',\n",
       " 'aus',\n",
       " 'austic',\n",
       " 'austin',\n",
       " 'austinairport',\n",
       " 'australia',\n",
       " 'austrian',\n",
       " 'author',\n",
       " 'authoritative',\n",
       " 'authority',\n",
       " 'authorize',\n",
       " 'auto',\n",
       " 'automate',\n",
       " 'automatically',\n",
       " 'automobile',\n",
       " 'autoresponse',\n",
       " 'av_duffy',\n",
       " 'avail',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'avatar',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'avert',\n",
       " 'avgeek',\n",
       " 'aviation',\n",
       " 'avios',\n",
       " 'avis',\n",
       " 'aviv',\n",
       " 'avoid',\n",
       " 'avon',\n",
       " 'avp',\n",
       " 'await',\n",
       " 'awaiting',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'aweful',\n",
       " 'awesome',\n",
       " 'awesomeee',\n",
       " 'awesomeness',\n",
       " 'awful',\n",
       " 'awfulness',\n",
       " 'awheelchair',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awrd',\n",
       " 'aww',\n",
       " 'awww',\n",
       " 'awwweesssooomee',\n",
       " 'ayyy',\n",
       " 'ba_usa',\n",
       " 'baby',\n",
       " 'bach',\n",
       " 'bachelorpartymishap',\n",
       " 'back',\n",
       " 'backing',\n",
       " 'backlog',\n",
       " 'backpack',\n",
       " 'backroads',\n",
       " 'backup',\n",
       " 'backwards',\n",
       " 'bad',\n",
       " 'badbadbad',\n",
       " 'badbusiness',\n",
       " 'badbussiness',\n",
       " 'badcustomerservice',\n",
       " 'badcustomersrvice',\n",
       " 'bademployeeproblem',\n",
       " 'badge',\n",
       " 'badges',\n",
       " 'badly',\n",
       " 'badmgmt',\n",
       " 'badpolicy',\n",
       " 'badservice',\n",
       " 'badwebsite',\n",
       " 'bae',\n",
       " 'baejet',\n",
       " 'bafore',\n",
       " 'baftz',\n",
       " 'bag',\n",
       " 'bagage',\n",
       " 'bagawim',\n",
       " 'baggage',\n",
       " 'baggagelost',\n",
       " 'bags',\n",
       " 'bahamas',\n",
       " 'bail',\n",
       " 'bailey',\n",
       " 'bait',\n",
       " 'baitandswitch',\n",
       " 'bake',\n",
       " 'baking',\n",
       " 'balance',\n",
       " 'baldordash',\n",
       " 'baldwin',\n",
       " 'ball',\n",
       " 'ballin',\n",
       " 'balls',\n",
       " 'baltimore',\n",
       " 'ban',\n",
       " 'banana',\n",
       " 'band',\n",
       " 'bandie',\n",
       " 'bandwidth',\n",
       " 'bangkok',\n",
       " 'bank',\n",
       " 'bankrupt',\n",
       " 'bankruptcy',\n",
       " 'bar',\n",
       " 'barbados',\n",
       " 'barbara',\n",
       " 'barclay',\n",
       " 'barclaycardus',\n",
       " 'barclays',\n",
       " 'barcodes',\n",
       " 'barely',\n",
       " 'barking',\n",
       " 'barklays',\n",
       " 'barnum',\n",
       " 'barrel',\n",
       " 'barrettkarabis',\n",
       " 'barrier',\n",
       " 'bars',\n",
       " 'barzegar',\n",
       " 'base',\n",
       " 'bashing',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basket',\n",
       " 'basketball',\n",
       " 'bass',\n",
       " 'bastard',\n",
       " 'bathroom',\n",
       " 'batman',\n",
       " 'battery',\n",
       " 'battierccipuppy',\n",
       " 'batting',\n",
       " 'battle',\n",
       " 'battles',\n",
       " 'battling',\n",
       " 'bay',\n",
       " 'bbb_media',\n",
       " 'bbbne_sd_ks_ia',\n",
       " 'bcn',\n",
       " 'bcuz',\n",
       " 'bday',\n",
       " 'bdindallas',\n",
       " 'bdl',\n",
       " 'bdng',\n",
       " 'bdsm',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beamske',\n",
       " 'bean',\n",
       " 'beanie',\n",
       " 'beantownmatty',\n",
       " 'bear',\n",
       " 'bearable',\n",
       " 'beat',\n",
       " 'beating',\n",
       " 'beatriz',\n",
       " 'beats',\n",
       " 'beatsmusic',\n",
       " 'beatstheothers',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'bed',\n",
       " 'bedofroses',\n",
       " 'beefjerky',\n",
       " 'beer',\n",
       " 'beers',\n",
       " 'befor',\n",
       " 'beg',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'begrudgingly',\n",
       " 'behalf',\n",
       " 'behave',\n",
       " 'behaves',\n",
       " 'behavior',\n",
       " 'behind',\n",
       " 'bein',\n",
       " 'beingsuckontarmacsucks',\n",
       " 'belabor',\n",
       " 'belfast',\n",
       " 'belfastairport',\n",
       " 'belief',\n",
       " 'believable',\n",
       " 'believe',\n",
       " 'belize',\n",
       " 'bellagio',\n",
       " 'belligerent',\n",
       " 'belong',\n",
       " 'belonging',\n",
       " 'beloved',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'benadryl',\n",
       " 'bench',\n",
       " 'bene',\n",
       " 'beneficial',\n",
       " 'benefit',\n",
       " 'benjaminokeefe',\n",
       " 'bensonhenderson',\n",
       " 'beought',\n",
       " 'bereavement',\n",
       " 'bergstrom',\n",
       " 'berlin',\n",
       " 'bernhardtjh',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'bestairline',\n",
       " 'bestairlineever',\n",
       " 'bestemployees',\n",
       " 'bestflightever',\n",
       " 'besty',\n",
       " 'bet',\n",
       " 'betsy',\n",
       " 'better',\n",
       " 'beverage',\n",
       " 'beware',\n",
       " 'beyond',\n",
       " 'bff',\n",
       " 'bgm',\n",
       " 'bgr',\n",
       " 'bhm',\n",
       " 'bicycle',\n",
       " 'big',\n",
       " 'biggest',\n",
       " 'biggie',\n",
       " 'bike',\n",
       " 'bila',\n",
       " 'bill',\n",
       " 'billing',\n",
       " 'billion',\n",
       " 'billmelate',\n",
       " 'bin',\n",
       " 'bingo',\n",
       " 'bins',\n",
       " 'bio',\n",
       " 'bird',\n",
       " 'birder',\n",
       " 'birmingham',\n",
       " 'birth',\n",
       " 'birthdate',\n",
       " 'birthday',\n",
       " 'biscuit',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bitchy',\n",
       " 'bite',\n",
       " 'bitty',\n",
       " 'biz',\n",
       " 'bizarre',\n",
       " 'biztravel',\n",
       " 'bked',\n",
       " 'bkk',\n",
       " 'black',\n",
       " 'blackhistorymonth',\n",
       " 'blacklist',\n",
       " 'blacklivesmatter',\n",
       " 'blackmailed',\n",
       " 'blade',\n",
       " 'blah',\n",
       " 'blame',\n",
       " 'blameshiftoverload',\n",
       " 'blanc',\n",
       " 'blank',\n",
       " 'blanket',\n",
       " 'blast',\n",
       " 'blasting',\n",
       " 'blatant',\n",
       " 'blatantly',\n",
       " 'blatimore',\n",
       " 'blegh',\n",
       " 'bleh',\n",
       " 'bless',\n",
       " 'blew',\n",
       " 'blind',\n",
       " 'blindside',\n",
       " 'bliss',\n",
       " 'blizzard',\n",
       " 'bloat',\n",
       " 'block',\n",
       " 'blog',\n",
       " 'blood',\n",
       " 'bloody',\n",
       " 'bloodymary',\n",
       " 'bloombergradio',\n",
       " 'blow',\n",
       " 'blowing',\n",
       " 'blown',\n",
       " 'blue',\n",
       " 'bluecarpet',\n",
       " 'bluemanity',\n",
       " 'bluetiful',\n",
       " 'blushing',\n",
       " 'bmi',\n",
       " 'bna',\n",
       " 'bnasnow',\n",
       " 'bng',\n",
       " 'board',\n",
       " 'boarded',\n",
       " 'boarding',\n",
       " 'boards',\n",
       " 'boat',\n",
       " 'bobbi',\n",
       " 'bobwesson',\n",
       " 'body',\n",
       " 'boeing',\n",
       " 'boeingairplanes',\n",
       " 'bogota',\n",
       " 'bohol',\n",
       " 'boil',\n",
       " 'boise',\n",
       " 'bold',\n",
       " 'boldflavors',\n",
       " 'bom',\n",
       " 'bonnie',\n",
       " 'bonsinthesky',\n",
       " 'bonus',\n",
       " 'boo',\n",
       " 'boofin',\n",
       " 'book',\n",
       " 'booked',\n",
       " 'booking',\n",
       " 'booklet',\n",
       " 'bookofnegroes',\n",
       " 'bool',\n",
       " 'boom',\n",
       " 'boooo',\n",
       " 'boost',\n",
       " 'boot',\n",
       " 'booze',\n",
       " 'bora',\n",
       " 'border',\n",
       " 'borderline',\n",
       " 'bored',\n",
       " 'bos',\n",
       " 'boston',\n",
       " 'bostonlogan',\n",
       " 'bot',\n",
       " 'bother',\n",
       " 'bottle',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'bougth',\n",
       " 'bounce',\n",
       " 'bound',\n",
       " 'bout',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'bqn',\n",
       " 'brag',\n",
       " 'brain',\n",
       " 'brand',\n",
       " 'brandloveaffair',\n",
       " 'brandmance',\n",
       " 'brandssayingbae',\n",
       " 'brave',\n",
       " 'brazil',\n",
       " 'breach',\n",
       " 'break',\n",
       " 'breakdown',\n",
       " 'breakfast',\n",
       " 'breath',\n",
       " 'bretharold',\n",
       " 'brian',\n",
       " 'bridge',\n",
       " 'bright',\n",
       " 'brilliant',\n",
       " 'bring',\n",
       " 'british_airways',\n",
       " 'bro',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brokenpromises',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'browser',\n",
       " 'bruh',\n",
       " 'brushing',\n",
       " 'brutal',\n",
       " 'btr',\n",
       " 'bttr',\n",
       " 'btv',\n",
       " 'btw',\n",
       " 'buck',\n",
       " 'bucket',\n",
       " 'buddy',\n",
       " 'budget',\n",
       " 'buenos',\n",
       " 'buf',\n",
       " 'buffalo',\n",
       " 'bug',\n",
       " 'bull',\n",
       " 'bullshit',\n",
       " 'bum',\n",
       " 'bummer',\n",
       " 'bump',\n",
       " 'bumped',\n",
       " 'bumper',\n",
       " 'bumping',\n",
       " 'bunch',\n",
       " 'burbank',\n",
       " 'burning',\n",
       " 'burningman',\n",
       " 'bus',\n",
       " 'bush',\n",
       " 'busiest',\n",
       " 'business',\n",
       " 'businessfirst',\n",
       " 'bussey',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'butnot',\n",
       " 'butt',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buyer',\n",
       " 'buying',\n",
       " 'buzz',\n",
       " 'bwahahaha',\n",
       " 'bwi',\n",
       " 'bwi_airport',\n",
       " 'bwood',\n",
       " 'bye',\n",
       " 'byebyejetblue',\n",
       " 'byod',\n",
       " 'bze',\n",
       " 'c_istudios',\n",
       " 'cab',\n",
       " 'cabcelled',\n",
       " 'cabin',\n",
       " 'cabine',\n",
       " 'cable',\n",
       " 'cabo',\n",
       " 'cac',\n",
       " 'cache',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3660x7000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 31492 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_features = count_vec.transform(test_text_documents)\n",
    "x_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=x_train_features\n",
    "ytrain=categories\n",
    "xtest=x_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anisha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [100.0, 1000.0, 5000.0, 10000.0, 50000.0, 100000.0], 'gamma': [0.001, 0.0005, 0.0001, 0.005]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svc=SVC(kernel=\"rbf\")\n",
    "grid={'C':[1e2,1e3,5e3,1e4,5e4,1e5],'gamma':[1e-3,5e-4,1e-4,5e-3]}\n",
    "abc=GridSearchCV(svc,grid)\n",
    "abc.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anisha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Anisha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Anisha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Anisha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Anisha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 6.16810036,  6.8836027 ,  7.18197584,  7.2391758 ,  7.72147179,\n",
       "         6.90481488,  6.02382119,  8.5236783 ,  9.98517799,  9.05411275,\n",
       "         6.88218331,  9.02098378, 10.95878967, 10.22845769,  7.71940279,\n",
       "         8.87591879, 13.27506653, 13.87338686, 10.78683956,  9.2880017 ,\n",
       "        13.90355825, 16.27112873, 14.04606263,  8.76593781]),\n",
       " 'std_fit_time': array([0.46915358, 0.55478221, 0.7511773 , 0.32724464, 0.31437583,\n",
       "        0.14244935, 0.04372076, 0.19898889, 0.39900811, 0.22508374,\n",
       "        0.09202093, 0.15425529, 0.66246323, 0.10041724, 0.21907091,\n",
       "        0.07071923, 0.71496422, 1.0269496 , 0.09986754, 0.40861914,\n",
       "        0.61040085, 2.02738104, 0.28707491, 0.18835501]),\n",
       " 'mean_score_time': array([2.25530195, 2.50315126, 2.94774143, 2.24383664, 2.07871675,\n",
       "        2.09425004, 2.28678179, 2.08673811, 1.91253948, 1.91331967,\n",
       "        2.07172163, 1.93278925, 1.84066502, 1.92319671, 1.94424725,\n",
       "        1.90443484, 1.81393313, 1.80379375, 1.9245611 , 1.93489138,\n",
       "        1.7607758 , 1.73620335, 1.79189897, 1.81424705]),\n",
       " 'std_score_time': array([0.0575205 , 0.18135011, 0.51240591, 0.01542866, 0.03490472,\n",
       "        0.01767769, 0.09863014, 0.10046141, 0.0128637 , 0.00064299,\n",
       "        0.04540431, 0.03549337, 0.01901486, 0.02848021, 0.02242192,\n",
       "        0.03114484, 0.05225784, 0.02617633, 0.08859462, 0.0982742 ,\n",
       "        0.05312606, 0.02023868, 0.00933001, 0.0196624 ]),\n",
       " 'param_C': masked_array(data=[100.0, 100.0, 100.0, 100.0, 1000.0, 1000.0, 1000.0,\n",
       "                    1000.0, 5000.0, 5000.0, 5000.0, 5000.0, 10000.0,\n",
       "                    10000.0, 10000.0, 10000.0, 50000.0, 50000.0, 50000.0,\n",
       "                    50000.0, 100000.0, 100000.0, 100000.0, 100000.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_gamma': masked_array(data=[0.001, 0.0005, 0.0001, 0.005, 0.001, 0.0005, 0.0001,\n",
       "                    0.005, 0.001, 0.0005, 0.0001, 0.005, 0.001, 0.0005,\n",
       "                    0.0001, 0.005, 0.001, 0.0005, 0.0001, 0.005, 0.001,\n",
       "                    0.0005, 0.0001, 0.005],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 100.0, 'gamma': 0.001},\n",
       "  {'C': 100.0, 'gamma': 0.0005},\n",
       "  {'C': 100.0, 'gamma': 0.0001},\n",
       "  {'C': 100.0, 'gamma': 0.005},\n",
       "  {'C': 1000.0, 'gamma': 0.001},\n",
       "  {'C': 1000.0, 'gamma': 0.0005},\n",
       "  {'C': 1000.0, 'gamma': 0.0001},\n",
       "  {'C': 1000.0, 'gamma': 0.005},\n",
       "  {'C': 5000.0, 'gamma': 0.001},\n",
       "  {'C': 5000.0, 'gamma': 0.0005},\n",
       "  {'C': 5000.0, 'gamma': 0.0001},\n",
       "  {'C': 5000.0, 'gamma': 0.005},\n",
       "  {'C': 10000.0, 'gamma': 0.001},\n",
       "  {'C': 10000.0, 'gamma': 0.0005},\n",
       "  {'C': 10000.0, 'gamma': 0.0001},\n",
       "  {'C': 10000.0, 'gamma': 0.005},\n",
       "  {'C': 50000.0, 'gamma': 0.001},\n",
       "  {'C': 50000.0, 'gamma': 0.0005},\n",
       "  {'C': 50000.0, 'gamma': 0.0001},\n",
       "  {'C': 50000.0, 'gamma': 0.005},\n",
       "  {'C': 100000.0, 'gamma': 0.001},\n",
       "  {'C': 100000.0, 'gamma': 0.0005},\n",
       "  {'C': 100000.0, 'gamma': 0.0001},\n",
       "  {'C': 100000.0, 'gamma': 0.005}],\n",
       " 'split0_test_score': array([0.77465173, 0.7689156 , 0.70854958, 0.76399891, 0.75143403,\n",
       "        0.76317946, 0.77383229, 0.74078121, 0.73613767, 0.74105436,\n",
       "        0.76263316, 0.7219339 , 0.73176728, 0.73285987, 0.75088774,\n",
       "        0.7219339 , 0.71100792, 0.71483201, 0.73149413, 0.71319312,\n",
       "        0.71046162, 0.70882273, 0.72357279, 0.71128107]),\n",
       " 'split1_test_score': array([0.76427206, 0.75962852, 0.71046162, 0.75853592, 0.74679049,\n",
       "        0.75279978, 0.76317946, 0.74160066, 0.73040153, 0.73804971,\n",
       "        0.75225348, 0.72767004, 0.73012838, 0.72767004, 0.7435127 ,\n",
       "        0.72685059, 0.71565146, 0.7230265 , 0.72630429, 0.71373942,\n",
       "        0.71565146, 0.71647091, 0.7186561 , 0.71155422]),\n",
       " 'split2_test_score': array([0.77610716, 0.76845271, 0.72006561, 0.76271186, 0.75369054,\n",
       "        0.75970476, 0.77446692, 0.73783488, 0.73701476, 0.73920175,\n",
       "        0.76216512, 0.72443958, 0.72033898, 0.73318753, 0.75041006,\n",
       "        0.72033898, 0.71104429, 0.71104429, 0.73100055, 0.71459814,\n",
       "        0.70721706, 0.70776381, 0.718152  , 0.71104429]),\n",
       " 'mean_test_score': array([0.77167577, 0.76566485, 0.71302368, 0.76174863, 0.75063752,\n",
       "        0.75856102, 0.7704918 , 0.74007286, 0.7345173 , 0.73943534,\n",
       "        0.75901639, 0.72468124, 0.72741348, 0.73123862, 0.74826958,\n",
       "        0.72304189, 0.71256831, 0.71630237, 0.72959927, 0.71384335,\n",
       "        0.71111111, 0.71102004, 0.7201275 , 0.71129326]),\n",
       " 'std_test_score': array([0.00526989, 0.00427338, 0.00503821, 0.00233215, 0.00287252,\n",
       "        0.00431448, 0.00517815, 0.00161684, 0.00293283, 0.00123787,\n",
       "        0.00478689, 0.00234831, 0.00504495, 0.00252743, 0.00336996,\n",
       "        0.00277127, 0.00218062, 0.0050007 , 0.00233908, 0.00057825,\n",
       "        0.0034736 , 0.0038793 , 0.00244536, 0.00020834]),\n",
       " 'rank_test_score': array([ 1,  3, 20,  4,  7,  6,  2,  9, 11, 10,  5, 15, 14, 12,  8, 16, 21,\n",
       "        18, 13, 19, 23, 24, 17, 22]),\n",
       " 'split0_train_score': array([0.88550348, 0.84465091, 0.72386938, 0.95887416, 0.9639295 ,\n",
       "        0.9483536 , 0.88413718, 0.99057248, 0.98251127, 0.97431343,\n",
       "        0.94767045, 0.99453477, 0.98824976, 0.98018855, 0.96188004,\n",
       "        0.99480803, 0.9933051 , 0.99070911, 0.97936877, 0.9950813 ,\n",
       "        0.99439814, 0.99248531, 0.98401421, 0.9950813 ]),\n",
       " 'split1_train_score': array([0.88564011, 0.85011614, 0.73275038, 0.95873753, 0.96720864,\n",
       "        0.94794371, 0.88427381, 0.98893291, 0.98360432, 0.97731931,\n",
       "        0.94630414, 0.99371499, 0.98647356, 0.98196475, 0.96461265,\n",
       "        0.99453477, 0.99234868, 0.98893291, 0.98046181, 0.9946714 ,\n",
       "        0.99412488, 0.99084574, 0.98360432, 0.99480803]),\n",
       " 'split2_train_score': array([0.88158973, 0.84334881, 0.72971866, 0.95943731, 0.9649003 ,\n",
       "        0.94728216, 0.88104343, 0.98962032, 0.98443048, 0.9756897 ,\n",
       "        0.94577984, 0.99385414, 0.98770828, 0.98251844, 0.96107621,\n",
       "        0.99453701, 0.99207867, 0.9886643 , 0.98019667, 0.99481016,\n",
       "        0.99330784, 0.99057635, 0.98402076, 0.99508331]),\n",
       " 'mean_train_score': array([0.88424444, 0.84603862, 0.72877947, 0.95901634, 0.96534614,\n",
       "        0.94785982, 0.88315147, 0.98970857, 0.98351536, 0.97577415,\n",
       "        0.94658481, 0.99403463, 0.9874772 , 0.98155725, 0.96252297,\n",
       "        0.99462661, 0.99257748, 0.98943544, 0.98000908, 0.99485429,\n",
       "        0.99394362, 0.99130247, 0.98387976, 0.99499088]),\n",
       " 'std_train_score': array([0.00187799, 0.00293183, 0.00368597, 0.00030286, 0.00137532,\n",
       "        0.00044142, 0.00149165, 0.00067225, 0.00078604, 0.0012286 ,\n",
       "        0.00079694, 0.00035819, 0.00074331, 0.00099386, 0.00151363,\n",
       "        0.00012829, 0.00052618, 0.00090727, 0.00046553, 0.00017022,\n",
       "        0.0004632 , 0.0008436 , 0.00019479, 0.00012929])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(C=100,kernel='rbf',gamma=0.001)\n",
    "svc.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'negative' 'negative' ... 'negative' 'positive' 'negative']\n"
     ]
    }
   ],
   "source": [
    "svc=svc.predict(xtest)\n",
    "svc=np.array(svc)\n",
    "print(svc)\n",
    "np.savetxt(\"twitter_pred_svc_rbf.csv\",svc, fmt='%s',encoding=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#MULTINOMIAL NAIVE BAYES SCORE TEST\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0.83)\n",
    "clf.fit(xtrain.toarray(), ytrain)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mnb=clf.predict(xtest)\n",
    "mnb=np.array(mnb)\n",
    "mnb\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"np.savetxt(\"twitter_pred_mnb.csv\",mnb, fmt='%s',encoding=None)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
